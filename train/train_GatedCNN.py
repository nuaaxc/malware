import torch
import random
import numpy as np
from tqdm import tqdm
import argparse
import torch.optim as optim
import torch.nn as nn
from models.GatedCNN import GatedCNN

from utils.optimizer import EarlyStopping
from preprocessing.read_data import data_loader
from sklearn import metrics

torch.manual_seed(2019)
random.seed(2019)
np.random.seed(2019)
tqdm.pandas(desc='Progress')


class TrainRevGrad:

    def __init__(self, opt, batch, vocab):
        self.device = torch.device(opt.device)
        self.opt = opt
        self.batch = batch
        self.best_checkpoint = None
        self.model = GatedCNN(seq_len=opt.seq_len,
                              vocab_size=len(vocab),
                              embed_size=self.opt.embed_size,
                              n_layers=self.opt.n_layers,
                              kernel=self.opt.kernel,
                              out_chs=self.opt.out_chs,
                              res_block_count=self.opt.res_block_count,
                              ans_size=self.opt.ans_size).to(self.device)

        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        self.loss_function = nn.NLLLoss().to(self.device)
        self.early_stopping = EarlyStopping(patience=opt.patience)

    def save_checkpoint(self, mean_stance_loss_dev, epoch_idx):
        checkpoint = {
            'loss': np.mean(mean_stance_loss_dev),
            'model': self.model.state_dict(),
            'epoch': epoch_idx,
            'save_mode': self.opt.save_mode
        }
        if self.opt.save_mode == 'all':
            # torch.save(checkpoint, self.model_path)
            self.best_checkpoint = checkpoint
            return False
        elif self.opt.save_mode == 'early':
            if self.early_stopping.step(checkpoint['loss'], checkpoint):
                # torch.save(self.early_stopping.get_best_model(), self.model_path)
                self.best_checkpoint = self.early_stopping.get_best_model()
                return True
            else:
                return False

    def train_on_batch(self, b):
        actions = b.ACTION
        y = b.LABEL

        y_pred = self.model(actions)
        loss = self.loss_function(y_pred, y)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()

    def train(self):
        train, dev, _ = self.batch

        for epoch in range(self.opt.n_epoch):
            # train
            self.model.train()
            mean_loss_train = []
            for b in train:
                loss = self.train_on_batch(b)
                mean_loss_train.append(loss)

            # validation
            self.model.eval()
            with torch.no_grad():
                mean_loss_dev = []
                mean_accuracy_dev = []
                for b in dev:
                    actions = b.ACTION
                    y = b.LABEL

                    y_pred = self.model(actions)
                    mean_loss_dev.append(self.loss_function(y_pred, y).item())
                    mean_accuracy_dev.append((y_pred.max(1)[1] == y).float().mean().item())

                tqdm.write(f'EPOCH {int(epoch+1):d}: '
                           f'| Train - loss={np.mean(mean_loss_train):.4f} '
                           f'| Dev - loss={np.mean(mean_loss_dev):.4f}, '
                           f'acc={np.mean(mean_accuracy_dev):.4f}')
                # checkpoint
                if self.save_checkpoint(mean_loss_dev, int(epoch)):
                    return

    def test(self):
        test = self.batch[-1]

        self.model.load_state_dict(self.best_checkpoint['model'])
        print('Done. dev_loss:', self.best_checkpoint['loss'], 'epoch_idx:', self.best_checkpoint['epoch'])

        self.model.eval()
        with torch.no_grad():
            y_true = []
            y_pred = []
            for b in tqdm(test):
                actions = b.ACTION
                y = b.LABEL

                y_ = self.model(actions)

                y_true.extend(y.cpu().numpy())
                y_pred.extend(y_.max(1)[1].cpu().numpy())

        return {
            'dev_loss': self.best_checkpoint['loss'],
            'epoch_idx': self.best_checkpoint['epoch'],
            'micro': metrics.f1_score(y_true, y_pred, average='micro'),
            'macro': metrics.f1_score(y_true, y_pred, average='macro'),
            'accuracy': metrics.accuracy_score(y_true, y_pred)
        }

    def run(self):
        # train
        if self.opt.train:
            self.train()

        # test
        if self.opt.test:
            return self.test()


if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    parser.add_argument('-train', action='store_true', help='train mode')
    parser.add_argument('-test', action='store_true', help='test mode')
    # params for train
    parser.add_argument('-device', type=str, default='cpu')
    parser.add_argument('-n_epoch', type=int, default=50)
    parser.add_argument('-batch_size', type=int, default=32)
    parser.add_argument('-save_mode', type=str, choices=['all', 'best', 'early'], default='early')
    parser.add_argument('-patience', type=int, default=5)
    parser.add_argument('-n_warmup_steps', type=int, default=10000)
    # params for model
    parser.add_argument('-embed_size', type=int, default=100)
    parser.add_argument('-seq_len', type=int, default=20)
    parser.add_argument('-n_layers', type=int, default=10)
    parser.add_argument('-kernel', type=int, default=5)
    parser.add_argument('-out_chs', type=int, default=64)
    parser.add_argument('-res_block_count', type=int, default=5)
    parser.add_argument('-ans_size', type=int, default=2)

    opt = parser.parse_args()

    batch, vocab = data_loader(opt)

    process = TrainRevGrad(opt, batch, vocab)

    results = process.run()

    print(results)
