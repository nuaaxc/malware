from torchtext import data
from config import DirConfig


def tokenizer(text):
    return text.split(':')


def data_loader(opt):
    TEXT = data.Field(sequential=True,
                      tokenize=tokenizer,
                      batch_first=True,
                      pad_first=False,
                      lower=True,
                      include_lengths=False,
                      pad_token='<pad>',
                      fix_length=opt.seq_len
                      )
    LABEL = data.Field(sequential=False, unk_token=None)

    fields = [
        ('APP_ID', None),
        ('LABEL', LABEL),
        ('ACTION', TEXT)]

    # read datasets
    print('reading data ...')
    train = data.TabularDataset(
        path=DirConfig.train_path,
        format='tsv',
        skip_header=True,
        fields=fields)

    dev = data.TabularDataset(
        path=DirConfig.dev_path,
        format='tsv',
        skip_header=True,
        fields=fields)

    test = data.TabularDataset(
        path=DirConfig.test_path,
        format='tsv',
        skip_header=True,
        fields=fields)

    TEXT.build_vocab(train, dev, test)
    LABEL.build_vocab(train)

    print('ACTION:')
    print('\tvocab size:', len(TEXT.vocab))

    print('LABEL:')
    print('\tvocab size:', len(LABEL.vocab))
    print('\t', LABEL.vocab.stoi.items())
    print('\t', LABEL.vocab.itos)

    print('\tDataset:')
    print('\t# Train:', len(train.examples))
    print('\t\tLABEL:', train.examples[0].LABEL)
    print('\t\tACTION:', train.examples[0].ACTION)

    print('\t# Dev:', len(dev.examples))
    print('\t\tLABEL:', dev.examples[0].LABEL)
    print('\t\tACTION:', dev.examples[0].ACTION)

    print('\t# Test:', len(test.examples))
    print('\t\tLABEL:', test.examples[0].LABEL)
    print('\t\tACTION:', test.examples[0].ACTION)

    print('=========================')

    batch = data.BucketIterator.splits(datasets=[train, dev, test],
                                       batch_sizes=[opt.batch_size] * 3,
                                       sort_key=lambda x: len(x.ACTION),
                                       device=opt.device,
                                       sort_within_batch=True,
                                       repeat=False)

    batch = [list(b) for b in batch]

    return batch, TEXT.vocab
